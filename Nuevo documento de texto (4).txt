from google.cloud import storage, bigquery
import pandas as pd
import functions_framework



def carga(data, project_id, dataset_id, table_id):
    """
    Función para cargar datos a BigQuery.
    """
    try:
        # Configuración de referencia de tabla
        bigquery_client = bigquery.Client(project=project_id)
        table_ref = f"{project_id}.{dataset_id}.{table_id}"

        # Configuración del trabajo de carga
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Sobrescribe los datos existentes
            source_format=bigquery.SourceFormat.PARQUET,
        )

        # Cargar datos
        job = bigquery_client.load_table_from_dataframe(data, table_ref, job_config=job_config)
        job.result()  # Espera a que el trabajo termine

        print(f"Datos cargados exitosamente en la tabla {table_ref}")

    except Exception as e:
        print(f"Error al cargar datos a BigQuery: {str(e)}")
        raise



@functions_framework.cloud_event
def hello_pubsub(cloud_event):
    try:
        bucket_name = 'us-central1-prueba-3b82ddb6-bucket'
        input_path = 'data/business-metadatos-inner.parquet'
        output_path = "output/transformed_data.csv"
        
        
        # Configuración de BigQuery
        project_id = 'hazel-framing-413123'
        dataset_id = 'datosfeastly'
        table_id = 'tablaproyecto'
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(input_path)

        # Descargar archivo Parquet a almacenamiento temporal
        temp_parquet_path = "/tmp/temp.parquet"
        blob.download_to_filename(temp_parquet_path)

        # Leer los datos del archivo Parquet
        data = pd.read_parquet(temp_parquet_path)

        # Realizar transformación
        data = data.drop(columns=['categories'])
        data = data.explode('category')


        # Cargar los datos a BigQuery
        carga(data, project_id, dataset_id, table_id)
        
        
        # Guardar como CSV
        #temp_csv_path = "/tmp/transformed_data.csv"
        #data.to_csv(temp_csv_path, index=False)

        # Subir CSV transformado a GCS
        #transformed_blob = bucket.blob(output_path)
        #transformed_blob.upload_from_filename(temp_csv_path)

        #print(f"Archivo transformado cargado exitosamente en: gs://{bucket_name}/{output_path}")

    except Exception as e:
        print(f"Error en la función: {str(e)}")
        raise  # Lanza la excepción para que GCP registre el fallo



